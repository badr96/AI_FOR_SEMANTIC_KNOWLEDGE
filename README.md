# <center> READ ME:</center>

## **Python 3.X with openjdk 11.0.3 2019-04-16 or higher is required to run this project**

## Usage: 

```
pip install -r requirements.txt
```
**once all the requirements are installed run the following script**

```
./script.sh
```

**This will generate three output and print some prepared Queries :**
  *  poi.csv generated by **data_scrapper.py**
  *  data_predicted.csv generated by **categoriser.py**
  *  monument.rdf generated by **RDF_generator.jar**
  *  Queries will be printed on terminal


## Deep Diving:

 ### **Introduction:**
  The aim of this project is to extract knowledge from data, firstly by applying a Machine Learning model to cluster data using K-means in my case and in a second hand to create a semantic knowledge and this will go through creating an RDF file, so we can run sparql queries on our semantic database.

 We will firstly begin with how we get data, then with the second part we will see K-means, and how we build our model, third step, which is creating an RDF file giving a csv file, and finaly apply some queries to test our semantic database.

 ### **1) Get the Data: "The scrapper"**
 Before any steps we can't go further without data, so i wrote a script **data_scrapper.py**, which is a script based on SPARQL queries to request dbpedia database. I was interseted by the monuments in Paris so I scrapped according to some ontlogies the monuments that I'm interested with, examples of ontologies *Museum, ArchitecturalStructure, Bridge ...* and this will generate a csv file **"poi.csv"**

 ### **2)Hands on the model: "with K-means"**
 So Here we are in the seconde part of the project, building a model, so we can apply prediction on the data scrapped. Since we have text data we have to vectorise it so can get it on a numeric format, which more easy for a computer :smiley:,So firstly some preproccessing are required, cleaning data with stops word, then applying some regex to remove all the useless character so on ..., then there come the vectorising data, in this step I proceed with *TF-IDF* which a frequency word counting <a href="https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76"> if you are more curious </a>. Once we have our vector we can apply K-means to create cluster, But wait how many cluster do u have *AHA*, to define this i choose one of the multiple methodes to choose the number of **K** here i proceed with the Kelbow methode so K for me was 14 and here we fit our model, generate a csv file **"predicted_data.csv"**, and we are done with the seconde part.

 PS: I didn't apply any validation methode, So as improvement feature I will suggeste to go with *Silhouette index or davies bouldin index*, so you can get an idea about the accuracy of the model. Second improvement feature is to choose another vectorising methode, here some other methode that I read about, *LDA2VEC, Bert, Word2vec*.

 ### **3)Semantic Knowledge: "RDF_GENERATOR"**
 So after building our model, now we will try to build a Semantic knowledge based on the predictive data from the last part. First step is to read our CSV and to creat our object, so we will read the csv line by line and construct our triplet  *Subject-Predicate-Object*
 this will generate an rdf file **"Monuments.rdf"**
 
 ### **4)SPARQL: Queries to test our rdf document**
 Here i run some simple queries just to test that my rdf schema is working 